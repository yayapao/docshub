---
id: sourcecode
title: ''
sidebar_label: Source Code
---

import { DocsHeader } from '/src/components/Layout/DocsHeader'
import {
  HighlightWithCode,
  HighlightWithText,
} from '/src/components/Highlights/TextContent'
import { ResolvedTag } from '/src/components/Highlights/CustomizedTag'

<DocsHeader
  title="Sentry Source Code"
  description="Read Sentry sourcecode step by step."
  tags={['sourcecode']}
  links={[
    {
      label: `Ingest consumer pipeline`,
      link: 'https://getsentry.github.io/event-ingestion-graph/',
    },
  ]}
/>{' '}

## Customized OpenAPI

:::note

如何自定义 OpenAPI？

:::

**Why need to customized openAPI?**

我们希望基于采集数据做 sentry 尚未完备的分析和探索，因此我们需要提供接口来关联数据、分析数据以及对外提供服务。

**为什么不直连 clickhouse 进行分析？**

- 权限，sentry 通过 API token 来解决 openAPI 的权限问题
- 架构复杂性，我们并不希望开启一个新的后端服务来做 sentry 内已兼备的工作
- 性能问题，CK 并不擅长做查询操作，并且 sentry 已经做了大量的工作来保证查询性能

Sentry 的 API 服务是基于 [django](https://www.djangoproject.com/) 实现。这里结合 sentry 对其基本点进行归纳：

- 路由配置位于 `src/sentry/api/urls.py`，遵循 `url(RegExp, View, name)` 的形式进行声明
- View 通常继承自 `[模块基类] > base.py > view.py`，以 project-event 为例，其位于 `src/sentry/api/endpoints/project_events.py`，类继承自 `ProjectEndpoint` 基类

## ingest-consumer

:::tip What's ingest-consumer?

The "ingest consumer" tasks read events from a kafka topic (coming from Relay) and schedules
process event celery tasks for them

:::

**ingest-consumer** 用于消费由 Relay 写入 `ingest-events` topic 的数据，并为其编排事件处理任务。它是 Sentry 开始处理任务的第一步，我们可以单独部署来启动 `ingest-consumer` 服务。

通过 `sentry run ingest-consumer --help` 查看该服务详情：

```sh

Usage: sentry run ingest-consumer [OPTIONS]

  Runs an "ingest consumer" task.

  The "ingest consumer" tasks read events from a kafka topic (coming from Relay) and schedules
  process event celery tasks for them

Options:
  --consumer-type [events|attachments|transactions]
                                  Specify which type of consumer to create, i.e. from which topic
                                  to consume messages. By default all ingest-related topics are
                                  consumed

  --all-consumer-types            Listen to all consumer types at once.
  --force-cluster TEXT            Kafka cluster ID of the overriden topic. Configure clusters via
                                  KAFKA_CLUSTERS in server settings.

  --force-topic TEXT              Override the Kafka topic the consumer will read from.
  --auto-offset-reset [earliest|latest|error]
                                  Position in the commit log topic to begin reading from when no
                                  prior offset has been recorded.

  --max-batch-time-ms INTEGER     How long to batch for before committing offsets.
  --max-batch-size INTEGER        How many messages to process before committing offsets.
  --consumer-group TEXT           Kafka consumer group for the consumer.
  --concurrency INTEGER           Thread pool size (only utilitized for message types that support
                                  concurrent processing)

```

其中：

- `--concurrency` 仅用于支持并发处理的消息类型
- 支持消费三种类型数据，我们可以根据实际情况来拆分服务。比如压力来自于 transctions，则可以通过 `--consumer-type` 将其独立部署

在开始解读源码之前，我们通过 [Event Ingestion Pipline](https://getsentry.github.io/event-ingestion-graph/) 可以发现：

- 在 Sentry 内，各 task 基于 Celery(使用 Redis) 进行通信，tasks 由 ingest-consumer 进行编排

<!-- 展示 event-ingest 视图 -->

<div
  style={{
    position: 'relative',
    width: '100%',
    height: '600px',
    overflow: 'hidden',
  }}
>
  <iframe
    width="100%"
    height="680"
    frameborder="0"
    scrolling="no"
    style={{ position: 'absolute', top: '-110px', left: '-10px' }}
    src="https://getsentry.github.io/event-ingestion-graph/"
  ></iframe>
</div>{' '}

### Start

我们以 `sentry run ingest-consumer --all-consumer-types --concurrency 16 --max-batch-size 100 --max-batch-time-ms 200` 作为命令入口，看看 Sentry 如何根据该命令启动服务。

#### 命令入口

`sentry/runner/commands/run.py` 作为入口文件，当接受到执行命令之后，会执行**初始化工作**：

- 判断支持消费事件类型
- 判断是否为 ThreadPoolExecutor(线程池处理器)，即并非处理
- 初始化 `ingest_consumer` 实例
- 执行其 `run()` 方法

### 获取 ingest_consumer 实例

我们通过 `get_ingest_consumer()` 方法获取 topic 的 consumer 实例。一般而言，实例化 Kafka 消费者需要：

- topics
- group name
- cluster name

![get_ingest_consumer](/img/fte/sentry/get_ingest_consumer.png)

在 `sentry/ingest/ingest_consumer.py` 内，Sentry 解析 consumer-type 参数获取 **topics**，并调用 `create_batching_kafka_consumer()` 进一步获取实例。同时，在此步骤还在初始化 `IngestConsumerWorker` 实例，它被用来进行 tasks 编排。

在 `sentry/utils/kafka.py` 内，Sentry 调用 `create_batching_kafka_consumer()` 方法，会先验证 topics 和配置的 cluster 是否合法。如果通过合法性检验，则继续返回 `BatchingKafkaConsumer()`

在 `sentry/utils/batching_kafka_consumer.py`，Sentry 定义创建消费者实例的**核心方法**：

- BatchingKafkaConsumer 是 `confluent_kafka.Consumer` 实现的进一步封装
- 通过 `BatchingKafkaConsumer.__init__()` 方法，定义对象变量
- 调用 `create_consumer()` 生成最终 consumer 并将其挂载至 `BatchingKafkaConsumer.consumer` 上

```py
# 创建 consumer 并订阅指定 topics，注意 Sentry 关闭了自动提交
def create_consumer():
    consumer_config = kafka_config.get_kafka_consumer_cluster_options(
        cluster_name,
        override_params={
            "enable.auto.commit": False,
            "group.id": group_id,
            "default.topic.config": {"auto.offset.reset": auto_offset_reset},
            # overridden to reduce memory usage when there's a large backlog
            "queued.max.messages.kbytes": queued_max_messages_kbytes,
            "queued.min.messages": queued_min_messages,
        },
    )
    consumer = Consumer(consumer_config)
    consumer.subscribe(
        topics, on_assign=on_partitions_assigned, on_revoke=on_partitions_revoked
    )

    return consumer
```

Sentry 实现的 `Class BatchingKafkaConsumer` 是多种 kafka 消费者处理事件循环的抽象，它与原生的 `KafkaConsumer` 几个不同点：

- 消费的消息会在本地处理，一旦拿到消息就会写入缓存的批处理中
- 根据批次的触发时间或者 size 来触发 flush
- **手动提交 offset**，这样做是为了减少数据丢失的概率（并不会减少重复消费的问题）。当一批数据发生 flush 时，会将其存储到外部存储节点，并立即提交 offset(在同一个线程/循环中)
- 支持 `dead letter topic` 来处理 `process_evet` 发生错误的情况，以避免发生阻塞

### 执行 BatchingKafkaConsumer.run

:::tip

`run()` 方法实际就是执行在实例化 consumer 时提供的 `Class IngestConsumerWorker` worker 实例所实现的事件循环。

:::

![ingest_consumer_run](/img/fte/sentry/ingest_consumer_run.png)

在 `sentry/utils/batching_kafka_consumer.py` 内，执行 `_run_once()` 方法，该方法会执行三个关键步骤：

- `self._flush()` 用来决定 BatchingKafkaConsumer 是否需要执行 flush，即清空当前 batch 并且提交 offset 到 kafka
- `msg = self.consumer.poll(timeout=1.0)`
- `self._handle_message(msg)`

在 `_flush()` 内会调用 `self.worker.flush_batch()`方法，之后依次执行 `self._commit()` 和 `self._reset_batch()`。

其中，`self.worker.flush_batch()` 为**核心方法**。它会调用 `sentry/ingest/ingest_consumer.py` 文件内的 `flush_batch()` 方法。

首先，执行 `mark_scope_as_unsafe()` 将 scope 显式标记为不安全，来允许 recursion breaker 在遍历堆栈寻找不安全文件之前尽早做出判断。设置完毕之后立即执行 `self._flush_batch` 方法，处理由 Kafka ingest-event topic 传递的批 message。

#### \_flush_batch

分别不同的事件类型，为其分配不同的处理器。

```py
def _flush_batch(self, batch: Sequence[Message]):
    projects_to_fetch = set()
    # 1. 将 message 的 project_id 写入 set 结构内；
    # 2. 根据 message 类型，对该批次 message 进行分类；
    # 3. 根据不同类型，将 message 与其对应的处理方法绑定，写入 list 结构内；
    with metrics.timer("ingest_consumer.prepare_messages"):
        for message in batch:
            message_type = message["type"]
            projects_to_fetch.add(message["project_id"])

            if message_type == "event":
                other_messages.append((self.__process_event, message))
            elif message_type == "attachment_chunk":
                attachment_chunks.append(message)
            elif message_type == "attachment":
                other_messages.append((process_individual_attachment, message))
            elif message_type == "user_report":
                other_messages.append((process_userreport, message))
            else:
                raise ValueError(f"Unknown message type: {message_type}")
    # 根据 project_id 从缓存内获取文件详情
    with metrics.timer("ingest_consumer.fetch_projects"):
        projects = {p.id: p for p in Project.objects.get_many_from_cache(projects_to_fetch)}

    # 遍历，将 message 作为参数执行对应的 process 方法
    # 这里声明 result 用来处理异步事件，当 reslut 内值被填充后，说明本次同步任务执行完毕
    # 继续遍历后，读取其 future 属性，继续执行其上一步约定的异步任务
    if other_messages:
        with metrics.timer("ingest_consumer.process_other_messages_batch"):
            results: MutableMapping["Future[Any]", "AsyncResult[Any]"] = {}
            # Execute synchronous tasks and dispatch asynchronous tasks.
            for processing_func, message in other_messages:
                result = processing_func(message, projects)
                # 为 AsyncResult 实例，则将其写入 future
                if isinstance(result, AsyncResult):
                    results[result.future] = result

            # 当所有同步任务执行完毕之后，再依次执行 future callback
            for future in as_completed(results.keys()):
                results[future].callback(future)

```

事件的执行器参考 `sentry/ingest/ingest_consumer.py` 内的 `process_event_async` 方法，其包含：

- `_load_event()`
- `_store_event()`
- `dispatch_task(key)`

#### load_event

- 做一些初始化过滤；
- 反序列化 message 并将其写入缓存，该份缓存能够被其他 task 使用；

可以看到其定义了 `dispatch_task` 方法，在该方法内能够调用 `preprocess_event` 方法。

```py
def _load_event(
    message: Message, projects: Mapping[int, Project]
) -> Optional[Tuple[Any, Callable[[str], None]]]:
    # ...

    # 当检查到重复的 message 后，直接返回
    deduplication_key = f"ev:{project_id}:{event_id}"
    if cache.get(deduplication_key) is not None:
        logger.warning(
            "pre-process-forwarder detected a duplicated event" " with id:%s for project:%s.",
            event_id,
            project_id,
        )
        return  # message already processed do not reprocess

    # 从 raw message 内解析 payload
    data = json.loads(payload)

    def dispatch_task(cache_key: str) -> None:
        # preprocess 预处理 event, 它产生 process_event 或者 save_event
        # 显式传输 data 避免再次从缓存中读取
        with sentry_sdk.start_span(op="ingest_consumer.process_event.preprocess_event"):
            preprocess_event(
                cache_key=cache_key,
                data=data,
                start_time=start_time,
                event_id=event_id,
                project=project,
            )

        # remember for an 1 hour that we saved this event (deduplication protection)
        cache.set(deduplication_key, "", CACHE_TIMEOUT)

        # emit event_accepted once everything is done
        event_accepted.send_robust(ip=remote_addr, data=data, project=project, sender=process_event)

    return data, dispatch_task
```

`_store_event(data)` 会将数据写入缓存，并返回其 key。参考 `sentry/eventstore/processing/base.py` 的 `store` 方法。

将返回 key 值作为参数调用 `dispatch_task` 方法，进一步执行 `preprocess_event` 方法。

#### preprocess_event

在该方法内，通过对 eventData 进行解析后，会存在三种处理方式：

- 被 symbolicate_event 处理后，执行 `submit_symbolicate`
- 被 process_event 处理后，执行 `submit_process`，在 process_event 内会执行 `submit_save_event`
- 事件未被前两张捕获，直接执行 `submit_save_event`，即将事件提交给 **Save envent Task** 处理

```py
def _do_preprocess_event(cache_key, data, start_time, event_id, process_task, project):
    # 前面获取了事件、项目以及组织信息

    if should_process_with_symbolicator(data):
        reprocessing2.backup_unprocessed_event(project=project, data=original_data)
        submit_symbolicate(
            project, from_reprocessing, cache_key, event_id, start_time, original_data
        )
        return

    if should_process(data):
        submit_process(
            project,
            from_reprocessing,
            cache_key,
            event_id,
            start_time,
            data_has_changed=False,
        )
        return

    submit_save_event(project, from_reprocessing, cache_key, event_id, start_time, original_data)

```

什么条件下 `should_process_with_symbolicator` 为 True ?

1. exception 的 `mechanism.type == minidump` 时；minidump 是包含崩溃进程的最重要内存区域文件。详细内容参考 [minidump](https://docs.sentry.io/platforms/native/guides/minidumps/)
2. exception 的 `mechanism.type == applecrashreport` 时；applecrashreport 表明该内容是 apple 的崩溃报告；
3. 判断 SDK 的 platform 是否命中 apple, symbolic, elf, macho, pe 和 wasm;

什么条件下 `should_process` 为 True ?

1. 事件类型不为 `transaction` 且事件或者事件的 stacktrace 能够支持所有插件的 `get_event_preprocessors` 和 `get_event_enhancers` 验证；

#### process_event

- 获取 event、project 以及组织信息，判断当前事件是否在上下文内（在，则不会继续处理）
- 解析并处理 stacktrace 数据
- 清洗数据内的敏感信息
- 依次执行 plugin 内的预处理函数来处理数据
- 对事件进行标准化处理，丢弃过大或者错误数据的堆栈信息
- 采取重试机制
- 执行 `save_event` Task

对于 `process_stacktraces` 来说，主要内容就是通过 `find_stacktraces_in_data` 解析 stacktrace，为其添加处理必须的元数据，之后调用 `get_stacktrace_processing_task` 来创建 frames 任务列表，最终调用 `process_single_stacktrace` 来处理每条 frames。

在得到处理后的 stacktrace 之后，Sentry 会立即调用 `scrub_data` 来清洗数据。

之后依次调用 `plugin.get_event_preprocessors` 来预处理被清洗后的数据。

处理完毕之后，Sentry 会判断 data 是否被改变，如果被改变了，则执行 `normalizer.normalize_event(dict(data))` 对其进行标准化处理，该标准遵循规则：

- 不会保留过大的堆栈
- 不会保存时间戳比 retention window 还大的堆栈

#### save_event

该任务的主要职责是将 event 写入数据库

- 删除还未被 preprocessing 的 事件和 data 为空的事件
- 判断 data 是否为空，为空则不会继续执行
- 调用 `EventManager.save` 方法，将被处理后的事件按照不同类型被处理后写入 pg 和 eventstream，**eventstream 内的事件被 snuba 和 post-processing 任务处理**

向 Pg 内写入相邻模型(save adjacent models such as releases and environments in postgres)，向 eventstream 和 TSDB 写入事件，在写入之前会按照事件类型，提取不同的信息，处理后进行写入。

snuba 能够处理重复写入，因此在 sentry 内直接插入就行，会以最后到达的事件为准。由于存在重复写入的问题，写入 eventstream 内的事件数量会大于最终的事件计数。

在 Sentry 内，有两种情况会导致 data 为空：

- data 在队列内过期，被清理掉了
- 在 reprocessing 时原生事件被删除了

### Q&A

#### 理解 batching-kafka-consumer 打印日志

1. `batching-kafka-consumer: Flushing 3 items (from {('ingest-events', 0): [152738642, 152738643], ('ingest-transactions', 0): [19053992, 19053992]}): forced:False size:False time:True`

查看 `batching_kafka_consumer.py`：

```py
def _flush(self, force=False):
    batch_by_size = len(self.__batch_results) >= self.max_batch_size
    batch_by_time = self.__batch_deadline and time.time() > self.__batch_deadline
    if not (force or batch_by_size or batch_by_time):
        return

    logger.info(
        "Flushing %s items (from %r): forced:%s size:%s time:%s",
        len(self.__batch_results),
        self.__batch_offsets,
        force,
        batch_by_size,
        batch_by_time,
    )
    # ...
```

通过关键代码块可以看出，日志用来打印：

- 本地处理批任务的项目个数
- 本地处理导致的 offset 变化
- 是不是强制执行以及本地 flush 触发条件（size or time）

上述日志可以读作：本批次处理 3 条数据，其中 2 条来自 `ingest-events`，1 条来自 `ingest-transactions`，由时间设置触发。

2. `batching-kafka-consumer: Worker flush took 97ms`

3. `[WARNING] sentry.ingest.ingest_consumer: pre-process-forwarder detected a duplicated event with id:2e31713080f3462fa48651f0b6446b32 for project:141`

这条日志表明，该项目的 event 已经被处理过，后续将不再重新处理。发生这种情况的原因可能是之前的 `forwarder` 实例处理完毕 event 之后，在提交 event queue offset 之前这段时间内挂了。

```py
def _load_event() {
    deduplication_key = f"ev:{project_id}:{event_id}"
    if cache.get(deduplication_key) is not None:
        logger.warning(
            "pre-process-forwarder detected a duplicated event" " with id:%s for project:%s.",
            event_id,
            project_id,
        )
        return  # message already processed do not reprocess
}
```

## SourceMap

:::note sourcemap
保存源码映射关系
:::

**Why sourcemap?**

在现代开发环境中，在应用上线到生产环境之前，我们往往需要对代码进行各种编译、优化处理，这将导致生产环境的代码不同于开发环境。尤其伴随着工程化的日益成熟，模块化已经是开发的常用方案。但是，处理后的代码会给我们在 debug 或者问题排障时会带来困扰。

因此需要一种机制，既能保证线上的发布流程，同时也能兼顾我们 debug 的需求，[sourcemap](https://docs.google.com/document/d/1U1RGAehQwRypUTovF1KRlpiOFze0b-_2gc6fAH0KY0k/edit?hl=en_US&pli=1&pli=1#heading=h.1ce2c87bpj24)应运而生。

Sentry 基于 sourcemap 提供了快速定位源码的能力。

sentry 会对 sourceFile 和 SourceMap File 单独进行处理，即 Sentry 会先从 stacktrace 内依次解析 frames，从中提取出源文件路径，拉取源文件后解析其 sourceMap 地址，再去拉取 SourceMap Files。

处理关键步骤在于如何缓存和读取文件内容。其中缓存文件时会使用到 `cache_key`(对应文件内容) 和 `cache_key_meta`(对应文件元信息)，而使用读取内容时会遵循**缓存 > database > internet**的顺序。

本章，我们会对 sentry 加工 frames 的过程进行阅读，并记录异常处理，以方便对生产问题进行排查。

![sourcemap_processor](/img/fte/sourcemap.svg)

### 判断 frames 是否有效

对于 stacktrace 内的 frames，通过遍历依次判断 frame 是否有效，**判断规则就是能够获取到 `lineno`**

```py

def is_valid_frame(frame):
    return frame is not None and frame.get("lineno") is not None

```

### 拉取文件

Sentry 提供 `fetch_file` 来拉取文件，Source File 和 SourceMap File 都通过该方法进行拉取。

拉取顺序遵循 `database(Event 内携带了 release 信息) > internet`

<HighlightWithText text="fetch_file()" />{' '}

```python
# url = abs_path
def fetch_file(url, project=None, release=None, dist=None, allow_scraping=True):
    # `frames.abs_path` 太长导致被省略号占位，不会继续请求
    if url[-3:] == "...":
        raise http.CannotFetch({"type": EventError.JS_MISSING_SOURCE, "url": http.expose_url(url)})

    # 如果存在 release，则优先去 Sentry 文件存储服务里面查找
    if release:
        result = fetch_release_artifact(url, release, dist)
    else:
        result = None

    # 生成 cache_key 用于标识 result
    cache_key = f"source:cache:v4:{md5_text(url).hexdigest()}"

    # 在 Sentry 文件存储服务里面没找到或者 release 没配置，则会尝试通过网络拉取
    # 此时需要保证开启了 `allow_scraping` 配置，并且 url 是以 http(s) 开头
    if result is None:
        if not allow_scraping or not url.startswith(("http:", "https:")):
            error = {"type": EventError.JS_MISSING_SOURCE, "url": http.expose_url(url)}
            raise http.CannotFetch(error)
        # 如果能够从缓存中获取，则将 body 解压之后写入 result
        result = cache.get(cache_key)
        if result is not None:
            result = http.UrlResult(
                result[0], result[1], zlib.decompress(result[2]), result[3], encoding
            )
    # 如果从缓存内获取失败，则从网络上进行拉取
    if result is None:
        headers = {}
        verify_ssl = False
        if project and is_valid_origin(url, project=project):
            verify_ssl = bool(project.get_option("sentry:verify_ssl", False))
            token = project.get_option("sentry:token")
            if token:
                token_header = project.get_option("sentry:token_header") or "X-Sentry-Token"
                headers[token_header] = token

        # 从网络上拉取文件，将 body 进行压缩后，将最终结果写入缓存
        with metrics.timer("sourcemaps.fetch"):
            result = http.fetch_file(url, headers=headers, verify_ssl=verify_ssl)
            z_body = zlib.compress(result.body)
            cache.set(
                cache_key,
                (url, result.headers, z_body, result.status, result.encoding),
                get_max_age(result.headers),
            )

            # 如果文件太大，则无法写入缓存，直接抛出异常
            if cache.get(cache_key) is None:
                error = {
                    "type": EventError.TOO_LARGE_FOR_CACHE,
                    "url": http.expose_url(url),
                }
                http.lock_domain(url, error=error)
                raise http.CannotFetch(error)

    # 请求文件失败
    if result.status != 200:
        raise http.CannotFetch(
            {
                "type": EventError.FETCH_INVALID_HTTP_CODE,
                "value": result.status,
                "url": http.expose_url(url),
            }
        )
    # 编码文件失败，Unable to fetch HTTP resource
    if not isinstance(result.body, bytes):
        try:
            result = http.UrlResult(
                result.url,
                result.headers,
                result.body.encode("utf8"),
                result.status,
                result.encoding,
            )
        except UnicodeEncodeError:
            error = {
                "type": EventError.FETCH_INVALID_ENCODING,
                "value": "utf8",
                "url": http.expose_url(url),
            }
            raise http.CannotFetch(error)

    # 如果不是 JavaScript/json 文件，Unable to fetch HTTP resource
    if urlsplit(url).path.endswith(".js"):
        body_start = result.body[:20].lstrip()
        if body_start[:1] == b"<":
            error = {"type": EventError.JS_INVALID_CONTENT, "url": url}
            raise http.CannotFetch(error)
    return result
```

### 处理源文件

处理完 frames 之后，将其移交给 `populate_source_cache` 来拉取所有我们需要的 Source File。

存在以下几种情况，不会去拉取源文件：

- `frames.abs_path` 太长导致被省略号占位
- 未配置 release 或者 release 文件为空的前提下，关闭 `Allow JavaScript Source Fetching` 或者 `frames.abs_path` 不是以 `http:` 或者 `https:` 开头
- 请求源文件失败
- 拿到源文件内容后，如果不是 bytes，需要对其进行编码，如果此步骤报错，返回 `FETCH_GENERIC_ERROR` 错误
- 拿到源文件内容后，如果不是 JavaScript/JSON 文件，则会返回 `FETCH_GENERIC_ERROR` 错误

<HighlightWithText text="populate_source_cache()" />{' '}

```python
def populate_source_cache(self, frames):
  pending_file_list = set()
  for f in frames:
    # 获取不到文件路径
    if f.get("abs_path") is None:
        continue
    # 有些 SDK 会上报，但是也不处理
    if f["abs_path"] == "<anonymous>":
        continue
    # SDK 平台为 node 并且文件路径不以 `app:` 开头
    if self.data.get("platform") == "node" and not f.get("abs_path").startswith("app:"):
        continue
    pending_file_list.add(f["abs_path"])
```

在 `populate_source_cache` 处理完 frames 的前置过滤之后，将请求文件列表提交给 `cache_source` 内进行处理，**它会请求 Source File 并缓存，并解析文件 SourceMappingURL**

<HighlightWithText text="cache_source()" />{' '}

```py
def cache_source(self, filename):
    self.fetch_count += 1
    # 每请求一次会自增一次，当大于最大请求数时，不会继续请求
    if self.fetch_count > self.max_fetches:
        cache.add_error(filename, {"type": EventError.JS_TOO_MANY_REMOTE_SOURCES})
        return

    # 请求源文件
    result = fetch_file(
        filename,
        project=self.project,
        release=self.release,
        dist=self.dist,
        allow_scraping=self.allow_scraping,
    )
    # 将请求文件内容写入缓存
    cache.add(filename, result.body, result.encoding)
    cache.alias(result.url, filename)
    # 根据源文件获取 SourceMappingURL
    sourcemap_url = discover_sourcemap(result)
    sourcemaps.link(filename, sourcemap_url)
    if sourcemap_url in sourcemaps:
        return

    # pull down sourcemap
    try:
        with sentry_sdk.start_span(
            op="JavaScriptStacktraceProcessor.cache_source.fetch_sourcemap"
        ) as span:
            span.set_data("sourcemap_url", sourcemap_url)
            sourcemap_view = fetch_sourcemap(
                sourcemap_url,
                project=self.project,
                release=self.release,
                dist=self.dist,
                allow_scraping=self.allow_scraping,
            )
    except http.BadSource as exc:
        # we don't perform the same check here as above, because if someone has
        # uploaded a node_modules file, which has a sourceMappingURL, they
        # presumably would like it mapped (and would like to know why it's not
        # working, if that's the case). If they're not looking for it to be
        # mapped, then they shouldn't be uploading the source file in the
        # first place.
        cache.add_error(filename, exc.data)
        return

    sourcemaps.add(sourcemap_url, sourcemap_view)

    # cache any inlined sources
    for src_id, source_name in sourcemap_view.iter_sources():
        source_view = sourcemap_view.get_sourceview(src_id)
        if source_view is not None:
            self.cache.add(non_standard_url_join(sourcemap_url, source_name), source_view)


```

### 处理 SourceMap

处理完源文件，并返回文件结果后，首先尝试用源文件内容中获取 sourcemap 索引地址，遵循 `header > force_bytes(sourcemap) > sourceMappingURL=` 的寻找顺序，如果找到，会将其 `filename-sourcemap_url` 键值对写入缓存，否则直接返回（不会报错）

<HighlightWithText text="discover_sourcemap()" />{' '}

```py
def discover_sourcemap(result):
    # 从 Fetch Source File Response 的 headers 里面获取 sourcemap，默认值为 `result.headers.get("x-sourcemap")`
    sourcemap = result.headers.get("sourcemap", result.headers.get("x-sourcemap"))

    # 将从 header 获取 sourcemap 值强制转换为字节
    sourcemap = force_bytes(sourcemap) if sourcemap is not None else None

    # 如果 Response Headers 里面没有携带 sourceMap 信息，则从文件内进行提取
    # Sentry 认定 SourceMappingURL 只会存在文件开头或者结尾，因此在解析 body 后直接提取前5行和最后5行内通
    # 遍历，匹配到 "//# sourceMappingURL=" 这样的内容后，直接截取其后面的内容作为 sourcemap
    if not sourcemap:
        parsed_body = result.body.split(b"\n")
        if len(parsed_body) > 10:
            possibilities = parsed_body[:5] + parsed_body[-5:]
        else:
            possibilities = parsed_body
        for line in possibilities:
            if line[:21] in (b"//# sourceMappingURL=", b"//@ sourceMappingURL="):
                sourcemap = line[21:].rstrip()
        if not sourcemap:
            search_space = possibilities[-1][-300:].rstrip()
            match = SOURCE_MAPPING_URL_RE.search(search_space)
            if match:
                sourcemap = match.group(1)

    if sourcemap:
        if b"/*" in sourcemap and sourcemap[-2:] == b"*/":
            index = sourcemap.index(b"/*")
            if index == 0:
                raise AssertionError(
                    "react-native comment found at bad location: %d, %r" % (index, sourcemap)
                )
            sourcemap = sourcemap[:index]
        sourcemap = non_standard_url_join(result.url, force_text(sourcemap))

    return force_text(sourcemap) if sourcemap is not None else None
```

之后，会根据 sourcemap_url 去调用 `fetch_file()` 方法请求指定 sourcemaps。因此在排障时我们可以从以下方面入手：

- 请求源文件内容
- `sourceMappingURL` 是否解析正确
- 生成 sourcemap 地址是否正确
- 请求 sourcemap 文件内容

值得注意的是生产 sourcemap url 的方法：

```python
_scheme_re = re.compile(r"^([a-zA-Z0-9-+]+://)(.*)$")


def non_standard_url_join(base, to_join):
    """A version of url join that can deal with unknown protocols."""
    if not to_join:
        return base

    match = _scheme_re.match(to_join)
    if match is not None:
        return to_join

    match = _scheme_re.match(base)
    if match is not None:
        original_base_scheme, rest = match.groups()
        base = "http://" + rest
    else:
        original_base_scheme = None

    rv = urljoin(base, to_join)
    if original_base_scheme is not None:
        match = _scheme_re.match(rv)
        if match is not None:
            return original_base_scheme + match.group(2)

    return rv
```
